import os
import json
import time
import traceback

from elasticsearch import Elasticsearch, helpers
from sentence_transformers import SentenceTransformer
from openai import OpenAI
from dotenv import load_dotenv

# -------------------------------
# 1. .env íŒŒì¼ ë¶ˆëŸ¬ì˜¤ê¸°
# -------------------------------
# .env íŒŒì¼ì´ code/ì— ìˆë“ , ìƒìœ„ í´ë”ì— ìˆë“  ìë™ ê²€ìƒ‰ë¨
load_dotenv()

ES_USERNAME = os.getenv("ES_USERNAME")
ES_PASSWORD = os.getenv("ES_PASSWORD")
ES_CA_CERT = os.getenv("ES_CA_CERT")   # ex) ./http_ca.crt
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
LLM_MODEL= "gpt-4o-mini" # gpt-4o-miniì—ì„œ ë³€ê²½

# í™˜ê²½ë³€ìˆ˜ í™•ì¸(ë””ë²„ê¹…ìš©)
print("[INFO] Loaded environment variables:")
print(f"ES_USERNAME: {ES_USERNAME}")
print(f"ES_PASSWORD: {'****' if ES_PASSWORD else None}")
print(f"ES_CA_CERT: {ES_CA_CERT}")
print(f"OPENAI_API_KEY: {'****' if OPENAI_API_KEY else None}")

# -------------------------------
# 2. SentenceTransformer ë¡œë“œ
# -------------------------------
model = SentenceTransformer("snunlp/KR-SBERT-V40K-klueNLI-augSTS")


def get_embedding(sentences):
    return model.encode(sentences)


def get_embeddings_in_batches(docs, batch_size=100):
    batch_embeddings = []
    for i in range(0, len(docs), batch_size):
        batch = docs[i:i + batch_size]
        contents = [doc["content"] for doc in batch]
        embeddings = get_embedding(contents)
        batch_embeddings.extend(embeddings)
        print(f"batch {i}")
    return batch_embeddings


# -------------------------------
# 3. Elasticsearch ì—°ê²° ì„¤ì •
# -------------------------------
# ES_CA_CERTëŠ” ì ˆëŒ€ê²½ë¡œ ë˜ëŠ” ìƒëŒ€ê²½ë¡œ ëª¨ë‘ í—ˆìš©ë©ë‹ˆë‹¤.
# ì˜ˆ: code/http_ca.crt ë˜ëŠ” C:/Users/.../http_ca.crt
ES_HOST = os.getenv("ES_HOST", "http://localhost:9200")

es = Elasticsearch(
    ES_HOST,
    request_timeout=30,
)

print(es.info())

# -------------------------------
# 4. ES ì¸ë±ìŠ¤ operation
# -------------------------------
def create_es_index(index, settings, mappings):
    if es.indices.exists(index=index):
        es.indices.delete(index=index)
    es.indices.create(index=index, settings=settings, mappings=mappings)


def delete_es_index(index):
    es.indices.delete(index=index)


def bulk_add(index, docs):
    actions = [{"_index": index, "_source": doc} for doc in docs]
    return helpers.bulk(es, actions)


# -------------------------------
# 4-1. sparse / dense ë¦¬íŠ¸ë¦¬ë²„
# -------------------------------
def sparse_retrieve(query_str, size):
    query = {
        "match": {
            "content": {
                "query": query_str
            }
        }
    }
    result = es.search(index="test", query=query, size=size, sort="_score")
    return result


def dense_retrieve(query_str, size):
    query_embedding = get_embedding([query_str])[0]
    knn = {
        "field": "embeddings",
        "query_vector": query_embedding.tolist(),
        "k": size,
        "num_candidates": 100
    }
    return es.search(index="test", knn=knn)


def hybrid_retrieve(query_str, size, alpha=0.5): # í•˜ì´ë¸Œë¦¬ë“œ í•¨ìˆ˜
    """
    sparse(BM25)ì™€ dense(KNN) ê²°ê³¼ë¥¼ ê°€ì¤‘ í•©ìœ¼ë¡œ ì„ëŠ” í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰.
    - alpha: sparse ê°€ì¤‘ì¹˜ (0~1). 0.5ë©´ ë™ë“±í•œ ë¹„ì¤‘.
    """
    # ê°ê° ê²€ìƒ‰
    sparse = sparse_retrieve(query_str, size)
    dense = dense_retrieve(query_str, size)

    combined = {}

    def normalized_and_add(results, weight):
        hits = results.get("hits", {}).get("hits", [])
        if not hits:
            return
        # ì ìˆ˜ ì •ê·œí™”
        max_score = max(h["_score"] for h in hits) or 1.0
        for h in hits:
            src = h.get("_source", {})
            docid = src.get("docid")
            if docid is None:
                # docid ì—†ìœ¼ë©´ ìŠ¤í‚µ
                continue
            norm_score = (h["_score"] / max_score) * weight
            if docid not in combined:
                combined[docid] = {
                    "_source": src,
                    "_score": 0.0,
                }
            combined[docid]["_score"] += norm_score

    # sparse / dense ê°ê° ë°˜ì˜
    normalized_and_add(sparse, alpha)
    normalized_and_add(dense, 1 - alpha)

    # ì ìˆ˜ ìˆœìœ¼ë¡œ ì •ë ¬ í›„ ìƒìœ„ sizeê°œ ì„ íƒ
    merged_hits = sorted(
        [
            {"_source": v["_source"], "_score": v["_score"]}
            for v in combined.values()
        ],
        key=lambda x: x["_score"],
        reverse=True,
    )[:size]

    # sparse_retrieveì™€ ë¹„ìŠ·í•œ í˜•íƒœë¡œ ë°˜í™˜
    return {"hits": {"hits": merged_hits}}


def rrf_fusion(result_list, k=60):
    """
    ì—¬ëŸ¬ ê²€ìƒ‰ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸ë¥¼ Reciprocal Rank Fusion(RRF)ìœ¼ë¡œ í•©ì¹˜ëŠ” ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜.

    Parameters
    ----------
    result_list : list[list[dict]]
        Elasticsearch search().get("hits", {}).get("hits", []) í˜•íƒœì˜ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸.
        ì˜ˆ: [sparse_hits, dense_hits]
    k : int
        RRF ìƒìˆ˜. ì¼ë°˜ì ìœ¼ë¡œ 10~60 ì‚¬ì´ ê°’ì„ ì‚¬ìš©.

    Returns
    -------
    dict
        {docid: {"_source": ..., "_score": rrf_score}} í˜•íƒœì˜ ë”•ì…”ë„ˆë¦¬
    """
    fused = {}

    for hits in result_list:
        if not hits:
            continue

        # ê° ë¦¬íŠ¸ë¦¬ë²„ì—ì„œì˜ ìˆœìœ„ (1ìœ„ë¶€í„° ì‹œì‘)
        for rank, h in enumerate(hits, start=1):
            src = h.get("_source", {})
            docid = src.get("docid")
            if docid is None:
                # docid ì—†ëŠ” ë¬¸ì„œëŠ” ìŠ¤í‚µ
                continue

            # RRF ì ìˆ˜: 1 / (k + rank)
            score = 1.0 / (k + rank)

            if docid not in fused:
                fused[docid] = {
                    "_source": src,
                    "_score": 0.0,
                }
            fused[docid]["_score"] += score

    return fused


def hybrid_retrieve_rrf(query_str, size, k=60, per_retriever_k=None):
    """
    BM25 ê¸°ë°˜ sparse ê²€ìƒ‰ê³¼ dense ë²¡í„° ê²€ìƒ‰ ê²°ê³¼ë¥¼
    Reciprocal Rank Fusion(RRF)ìœ¼ë¡œ í•©ì¹˜ëŠ” í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ í•¨ìˆ˜.

    Parameters
    ----------
    query_str : str
        ê²€ìƒ‰ ì§ˆì˜ ë¬¸ìì—´
    size : int
        ìµœì¢…ìœ¼ë¡œ ë°˜í™˜í•  ë¬¸ì„œ ìˆ˜
    k : int
        RRF ìƒìˆ˜ (ê¸°ë³¸ 60)
    per_retriever_k : int or None
        ê° ë¦¬íŠ¸ë¦¬ë²„ì—ì„œ ê°€ì ¸ì˜¬ ìƒìœ„ ë¬¸ì„œ ìˆ˜.
        Noneì´ë©´ sizeì™€ ë™ì¼í•˜ê²Œ ì‚¬ìš©.
    """
    if per_retriever_k is None:
        # ê²€ìƒ‰ í’€ì„ ë„“ê²Œ ê°€ì ¸ì™”ë‹¤ê°€ RRFë¡œ ì¬ì •ë ¬
        per_retriever_k = max(size, 50)

    # 1) ê°œë³„ ë¦¬íŠ¸ë¦¬ë²„ ì‹¤í–‰
    sparse = sparse_retrieve(query_str, per_retriever_k)
    dense = dense_retrieve(query_str, per_retriever_k)

    sparse_hits = sparse.get("hits", {}).get("hits", [])
    dense_hits = dense.get("hits", {}).get("hits", [])

    # 2) RRF ì ìˆ˜ ê³„ì‚°
    fused = rrf_fusion([sparse_hits, dense_hits], k=k)

    # 3) ì ìˆ˜ ìˆœìœ¼ë¡œ ì •ë ¬ í›„ ìƒìœ„ sizeê°œ ì„ íƒ
    merged_hits = sorted(
        [
            {"_source": v["_source"], "_score": v["_score"]}
            for v in fused.values()
        ],
        key=lambda x: x["_score"],
        reverse=True,
    )[:size]

    # 4) ê¸°ì¡´ sparse_retrieve / hybrid_retrieveì™€ ë™ì¼í•œ í˜•ì‹ìœ¼ë¡œ ë°˜í™˜
    return {"hits": {"hits": merged_hits}}


# -------------------------------
# 5. Elasticsearch ì¸ë±ìŠ¤ ì„¤ì •
# -------------------------------
settings = {
    "analysis": {
        "analyzer": {
            "nori": {
                "type": "custom",
                "tokenizer": "nori_tokenizer",
                "decompound_mode": "mixed",
                "filter": ["nori_posfilter"]
            }
        },
        "filter": {
            "nori_posfilter": {
                "type": "nori_part_of_speech",
                "stoptags": ["E", "J", "SC", "SE", "SF", "VCN", "VCP", "VX"]
            }
        }
    }
}

mappings = {
    "properties": {
        "docid": {
            "type": "keyword"
        },
        "src": {
            "type": "keyword"
        },
        "content": {
            "type": "text",
            "analyzer": "nori",
            "fields": {
                "keyword": {
                    "type": "keyword"
                }
            }
        },
        "embeddings": {
            "type": "dense_vector",
            "dims": 768,
            "index": True,
            "similarity": "l2_norm"
        }
    }
}

# -------------------------------
# 6. ì¸ë±ìŠ¤ ìƒì„±
# -------------------------------
create_es_index("test", settings, mappings)

# -------------------------------
# 7. ë°ì´í„° ë¡œë“œ ë° ì„ë² ë”©
# -------------------------------
index_docs = []
with open("./data/documents.jsonl", encoding="utf-8") as f:
    docs = [json.loads(line) for line in f]

embeddings = get_embeddings_in_batches(docs)

for doc, embedding in zip(docs, embeddings):
    doc["embeddings"] = embedding.tolist()
    index_docs.append(doc)

ret = bulk_add("test", index_docs)
print(ret)


# -------------------------------
# 8. RAG êµ¬í˜„
# -------------------------------
os.environ["OPENAI_API_KEY"] = OPENAI_API_KEY # type: ignore
client = OpenAI(
    timeout=10
)

llm_model = LLM_MODEL

persona_function_calling = """
ë‹¹ì‹ ì€ ê²€ìƒ‰ ì¦ê°• ìƒì„±(Retrieval-Augmented Generation, RAG) ì‹œìŠ¤í…œì„ ìœ„í•œ ì§ˆì˜ ë³€í™˜ ë„ìš°ë¯¸ì…ë‹ˆë‹¤.
ì‚¬ìš©ìì˜ ì§ˆë¬¸ê³¼ ì´ì „ ëŒ€í™” ë§¥ë½ì„ ì½ê³ , ê²€ìƒ‰ì— ìµœì ì¸ 'ë…ë¦½ ì§ˆì˜(standalone_query)'ë¥¼ ë§Œë“¤ì–´ì•¼ í•©ë‹ˆë‹¤.

[ì—­í• ]
- ì‚¬ìš©ìì˜ ì‹¤ì œ ì˜ë„ë¥¼ íŒŒì•…í•˜ì—¬, ê²€ìƒ‰ ì—”ì§„ì— ë„£ì„ ìˆ˜ ìˆëŠ” í•œ ë¬¸ì¥ì˜ í•œêµ­ì–´ ì§ˆì˜ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
- ëª¨í˜¸í•œ ëŒ€ëª…ì‚¬ë‚˜ ì§€ì‹œì–´ëŠ” ëª¨ë‘ êµ¬ì²´ì ì¸ ëª…ì‚¬/ê°œë…/ì¸ë¬¼ëª…ìœ¼ë¡œ ì¹˜í™˜í•´ì•¼ í•©ë‹ˆë‹¤.
  - ì˜ˆ: "ê·¸ ì‚¬ëŒ" â†’ "ì•Œë² ë¥´íŠ¸ ì•„ì¸ìŠˆíƒ€ì¸"
  - ì˜ˆ: "ì´ ì‚¬ê±´" â†’ "ì›Œí„°ê²Œì´íŠ¸ ì‚¬ê±´"
- í•„ìš”í•˜ë‹¤ë©´, ì˜ì–´ ê³ ìœ ëª…ì‚¬(ì¸ëª…, ì§€ëª…, ì´ë¡ ëª… ë“±)ë¥¼ í•¨ê»˜ ë³‘ê¸°í•©ë‹ˆë‹¤.

[ì…ë ¥ í˜•ì‹]
- msg: userì™€ assistantì˜ ëŒ€í™” íˆìŠ¤í† ë¦¬ ì „ì²´ (ë¦¬ìŠ¤íŠ¸ í˜•íƒœ)
  - ê° ì›ì†ŒëŠ” {"role": "user" or "assistant", "content": "..."} êµ¬ì¡°ì…ë‹ˆë‹¤.

[ì¶œë ¥ í˜•ì‹]
- JSON ê°ì²´ í˜•íƒœë¡œ ë‹¤ìŒ í•œ ê°€ì§€ë§Œ í¬í•¨í•˜ì„¸ìš”.
  - "standalone_query": (ê²€ìƒ‰ì— ì‚¬ìš©í•  í•œ ë¬¸ì¥ì˜ í•œêµ­ì–´ ì§ˆì˜)

[ì£¼ì˜ì‚¬í•­]
- "standalone_query"ëŠ” ì ˆëŒ€ ê³µë°± ë¬¸ìì—´ì´ ë˜ë©´ ì•ˆ ë©ë‹ˆë‹¤.
- ë‹µë³€ì—ëŠ” JSON ì´ì™¸ì˜ ë‹¤ë¥¸ í…ìŠ¤íŠ¸ë¥¼ í¬í•¨í•˜ì§€ ë§ˆì„¸ìš”.
"""


tools = [{
    "type": "function",
    "function": {
        "name": "standalone_query",
        "description": "ëŒ€í™” ë§¥ë½ì„ ë°˜ì˜í•˜ì—¬, ê²€ìƒ‰ ì—”ì§„ì— ë„£ê¸° ì¢‹ì€ í•œ ë¬¸ì¥ì˜ ì§ˆì˜ë¬¸ì„ ìƒì„±í•©ë‹ˆë‹¤.",
        "parameters": {
            "type": "object",
            "properties": {
                "standalone_query": {
                    "type": "string",
                    "description": "ê²€ìƒ‰ì— ì‚¬ìš©í•  ìµœì¢… í•œ ë¬¸ì¥ì˜ í•œêµ­ì–´ ì§ˆì˜"
                }
            },
            "required": ["standalone_query"]
        }
    }
}]


qa_persona = """
ë‹¹ì‹ ì€ í•œêµ­ì–´ ê³¼í•™Â·ìƒì‹ ì§ˆë¬¸ì— ë‹µí•˜ëŠ” ì „ë¬¸ Q&A ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤.
ë‹¹ì‹ ì—ê²ŒëŠ” ê²€ìƒ‰ ì‹œìŠ¤í…œìœ¼ë¡œë¶€í„° ê°€ì ¸ì˜¨ ë¬¸ì„œ ì¡°ê°(retrieved_context)ì´ ì£¼ì–´ì§€ë©°,
ë°˜ë“œì‹œ ì´ ë¬¸ì„œë“¤ì˜ ë‚´ìš©ì„ ìš°ì„ ì ìœ¼ë¡œ í™œìš©í•˜ì—¬ ë‹µë³€í•´ì•¼ í•©ë‹ˆë‹¤.

[ì…ë ¥ ì„¤ëª…]
- msg: ì‚¬ìš©ìì™€ì˜ ì „ì²´ ëŒ€í™” íˆìŠ¤í† ë¦¬ì…ë‹ˆë‹¤.
- retrieved_context: ê²€ìƒ‰ê¸°ë¡œë¶€í„° ê°€ì ¸ì˜¨ ë¬¸ì„œ ëª©ë¡ì…ë‹ˆë‹¤.
  - ê° ë¬¸ì„œëŠ” ë¬¸ë‹¨ í˜•íƒœì˜ í…ìŠ¤íŠ¸ì´ë©°, ko_MMLU, ARC ë“± ì‹œí—˜/í€´ì¦ˆì—ì„œ ì¶”ì¶œëœ ê³¼í•™Â·ìƒì‹ ì§€ì‹ì…ë‹ˆë‹¤.
  - ë‚´ìš©ì€ ë¬¼ë¦¬, í™”í•™, ìƒë¬¼, ì§€êµ¬ê³¼í•™, ì¸ë¬¼, ì—­ì‚¬, ì‚¬íšŒ ìƒì‹ ë“±ì…ë‹ˆë‹¤.

[ë‹µë³€ ìƒì„± ì›ì¹™]
1. ê²€ìƒ‰ ë¬¸ì„œ ìš°ì„ 
   - ê°€ëŠ¥í•œ í•œ retrieved_contextì— ìˆëŠ” ë‚´ìš©ì— ê¸°ë°˜í•´ì„œë§Œ ë‹µë³€í•˜ì„¸ìš”.
   - ë‹¹ì‹ ì´ ì‚¬ì „ ì§€ì‹ìœ¼ë¡œ ì•Œê³  ìˆë”ë¼ë„, ì½”í¼ìŠ¤ ë‚´ìš©ê³¼ ì¶©ëŒí•˜ë©´ ì½”í¼ìŠ¤ë¥¼ ìš°ì„ í•©ë‹ˆë‹¤.
   - ì½”í¼ìŠ¤ì— ëª…ì‹œëœ ì‚¬ì‹¤ì´ ìˆìœ¼ë©´, ê·¸ ë‚´ìš©ì„ ì¤‘ì‹¬ìœ¼ë¡œ ì •ë¦¬í•´ì„œ ì„¤ëª…í•˜ì„¸ìš”.

2. ì‚¬ì‹¤ì„± & ì •ì§ì„±
   - ë¬¸ì„œë“¤ ì–´ë””ì—ë„ ì •ë³´ê°€ ì—†ê±°ë‚˜, ë‚´ìš©ì´ ë„ˆë¬´ ë¶€ì¡±í•´ì„œ í™•ì‹ í•  ìˆ˜ ì—†ë‹¤ë©´:
     - ì§€ì–´ë‚´ì§€ ë§ê³ , ëª¨ë¥¸ë‹¤ê³  ì†”ì§íˆ ë§í•œ ë’¤,
     - ì½”í¼ìŠ¤ì—ì„œ ì•Œ ìˆ˜ ìˆëŠ” ë²”ìœ„(ì˜ˆ: ì¼ë°˜ì ì¸ ê²½í–¥, ì •ì˜ ìˆ˜ì¤€)ê¹Œì§€ë§Œ ì„¤ëª…í•˜ì„¸ìš”.
   - ì˜ˆì‹œ:
     - "ì œê³µëœ ìë£Œì—ëŠ” Xì— ëŒ€í•œ êµ¬ì²´ì ì¸ ë‚´ìš©ì€ ì—†ì§€ë§Œ, ì¼ë°˜ì ìœ¼ë¡œëŠ” ..."
     - "ê²€ìƒ‰ëœ ë¬¸ì„œë§Œìœ¼ë¡œëŠ” Yì— ëŒ€í•´ í™•ì‹¤íˆ ë§í•˜ê¸° ì–´ë µìŠµë‹ˆë‹¤. ë‹¤ë§Œ, ..."

3. ë©€í‹°í„´ ë§¥ë½ ë°˜ì˜
   - msg ì „ì²´ë¥¼ ë³´ê³  ì‚¬ìš©ìì˜ ì‹¤ì œ ì§ˆë¬¸ì´ ë¬´ì—‡ì¸ì§€ ì´í•´í•´ì•¼ í•©ë‹ˆë‹¤.
   - ì´ì „ ë°œí™”ì˜ ê°ì •/ì˜ë„(ê±±ì •, í˜¸ê¸°ì‹¬ ë“±)ë¥¼ ê°€ë³ê²Œ ë°˜ì˜í•˜ë©´ ì¢‹ìŠµë‹ˆë‹¤.
     - ì˜ˆ: "ê¸°ì–µ ìƒì‹¤ì¦ì´ ë¬´ì„­ê²Œ ëŠê»´ì§ˆ ìˆ˜ ìˆì–´ìš”. ìë£Œì— ë”°ë¥´ë©´, ì£¼ìš” ì›ì¸ì€ â€¦"

4. ë‹µë³€ ìŠ¤íƒ€ì¼
   - í•œêµ­ì–´ë¡œ ì¹œì ˆí•˜ê³  ëª…í™•í•˜ê²Œ ì„¤ëª…í•©ë‹ˆë‹¤.
   - ê¸°ë³¸ì€ 3~6ë¬¸ì¥ ì •ë„ì˜ ë‹¨ë½ìœ¼ë¡œ ë‹µí•˜ê³ , í•„ìš”í•˜ë©´ ì§§ì€ ëª©ë¡ì„ ì‚¬ìš©í•˜ì„¸ìš”.
   - í•µì‹¬ ì •ë³´ â†’ ì´ìœ /ê·¼ê±° â†’ ê°„ë‹¨í•œ ì •ë¦¬ ìˆœì„œë¥¼ ì§€í–¥í•©ë‹ˆë‹¤.
   - ìˆ˜ì¹˜/ì—°ë„/ì „ë¬¸ ìš©ì–´ëŠ” ê°€ëŠ¥í•˜ë©´ êµ¬ì²´ì ìœ¼ë¡œ ì œì‹œí•©ë‹ˆë‹¤.

[ì¶œë ¥ í˜•ì‹]
- í•œêµ­ì–´ ìì—°ë¬¸ìœ¼ë¡œë§Œ ë‹µë³€í•©ë‹ˆë‹¤. ë¶ˆí•„ìš”í•œ ë©”íƒ€ ì„¤ëª…(ì˜ˆ: "ë‹¤ìŒì€ ë‹µë³€ì…ë‹ˆë‹¤")ì€ ë„£ì§€ ë§ˆì„¸ìš”.
"""


def safe_chat_completion(
    max_retries=3,
    backoff_base=2,
    **kwargs
):
    """
    OpenAI ChatCompletion í˜¸ì¶œ ì‹œ ì˜ˆì™¸ë¥¼ ìºì¹˜í•˜ê³ 
    ì§€ìˆ˜ ë°±ì˜¤í”„ë¡œ ì—¬ëŸ¬ ë²ˆ ì¬ì‹œë„í•˜ëŠ” ë˜í¼ í•¨ìˆ˜.
    """
    for attempt in range(1, max_retries + 1):
        try:
            return client.chat.completions.create(**kwargs)
        except Exception as e:
            print(f"[WARN] OpenAI API ì‹¤íŒ¨ (ì‹œë„ {attempt}/{max_retries}): {e}")
            if attempt == max_retries:
                print("[ERROR] OpenAI API ì—°ì† ì‹¤íŒ¨, ì´ ìƒ˜í”Œì€ ë¹ˆ ë‹µë³€ìœ¼ë¡œ ë„˜ì–´ê°‘ë‹ˆë‹¤.")
                return None
            sleep_sec = backoff_base ** (attempt - 1)
            print(f"[INFO] {sleep_sec}ì´ˆ ëŒ€ê¸° í›„ ì¬ì‹œë„...")
            time.sleep(sleep_sec)

def answer_question(messages):
    response = {"standalone_query": "", "topk": [], "references": [], "answer": ""}

    msg = [{"role": "system", "content": persona_function_calling}] + messages

    # ğŸ”¹ timeout ì¡°ê¸ˆ ëŠ˜ë¦¬ê³ , safe_chat_completion ì‚¬ìš©
    result = safe_chat_completion(
        model=llm_model,
        messages=msg,
        tools=tools,  # type: ignore
        temperature=0,  # gpt5 x
        seed=1,
        timeout=20,   # 10 â†’ 20ì´ˆ ì •ë„ë¡œ ì—¬ìœ 
        max_retries=3
    )

    # ì—°ì† ì‹¤íŒ¨í•œ ê²½ìš°
    if result is None:
        response["answer"] = ""
        return response

    if result.choices[0].message.tool_calls:
        tool_call = result.choices[0].message.tool_calls[0]
        function_args = json.loads(tool_call.function.arguments) # type: ignore
        standalone_query = function_args.get("standalone_query")

        # RRF ê¸°ë°˜ í•˜ì´ë¸Œë¦¬ë“œ ë¦¬íŠ¸ë¦¬ë²„ ì‚¬ìš©
        search_result = hybrid_retrieve_rrf(standalone_query, 3, k=60)
        # search_result = hybrid_retrieve(standalone_query, 3, alpha=0.5)
        response["standalone_query"] = standalone_query

        documents = search_result["hits"]["hits"]
        retrieved_context = []
        references = []
        for doc in documents:
            content = doc["_source"]["content"]
            docid = doc["_source"]["docid"]
            src = doc["_source"]["src"]
            references.append({"docid": docid, "src": src})
            retrieved_context.append(content)

        qa_msg = [
            {"role": "system", "content": qa_persona},
            {
                "role": "user",
                "content": json.dumps(
                    {
                        "msg": messages,
                        "retrieved_context": retrieved_context,
                    },
                    ensure_ascii=False,
                ),
            },
        ]
        qaresult = safe_chat_completion(
            model=llm_model,
            messages=qa_msg,
            temperature=0,  # gpt5 x
            seed=1,
            timeout=20,
            max_retries=3,
        )

        response["topk"] = [doc["_source"]["docid"] for doc in documents]
        response["references"] = references

        if qaresult is None:
            response["answer"] = ""
        else:
            response["answer"] = qaresult.choices[0].message.content
    else:
        response["answer"] = result.choices[0].message.content

    return response


def eval_rag(eval_filename, output_filename):
    with open(eval_filename, encoding="utf-8") as f, open(output_filename, "w", encoding="utf-8") as of:
        idx = 0
        for line in f:
            j = json.loads(line)
            print(f'Test {idx}\nQuestion: {j["msg"]}')
            response = answer_question(j["msg"])
            print(f'Answer: {response["answer"]}\n')

            output = {
                "eval_id": j["eval_id"],
                "standalone_query": response["standalone_query"],
                "topk": response["topk"],
                "answer": response["answer"],
                "references": response["references"]
            }
            of.write(json.dumps(output, ensure_ascii=False) + "\n")
            idx += 1


eval_rag("./data/eval.jsonl", "sample_submission_hybrid2.csv")
